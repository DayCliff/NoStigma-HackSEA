{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5e551c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Starting Test Pre-Pruning=================\n",
      "Accuracy:   96.69%\n",
      "=================End Test=================\n",
      "=================Starting Test Post-Pruning=================\n",
      "Accuracy:   96.69%\n",
      "=================End Test=================\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def read_file(file_path) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def IG(target_feature, D, remove=\"null\") -> dict:\n",
    "    \"\"\"\n",
    "    Calculates information gain by partitioning D on features and calculating their entropy, then summing it all together\n",
    "    and subtracting that from the target feature entropy\n",
    "    @param target_feature: string, The desired outcome\n",
    "    @param D: dataframe, dataset needed to perform entropy calculations\n",
    "    @param remove: string, name of column needed to be removed from dataframe before entropy calculations. Optional\n",
    "    @return: the highest Information Gain feature\n",
    "    \"\"\"\n",
    "    IG_dict = {}\n",
    "    # partition on target feature\n",
    "    # calculate target feature entropy\n",
    "    if remove != \"null\":\n",
    "        D.drop(columns=['Day'], axis=1, inplace=True)\n",
    "    tf_entropy = calculate_entropy(target_feature, D)\n",
    "\n",
    "    feature_list = list(D.columns)\n",
    "\n",
    "    feature_list.remove(target_feature)\n",
    "\n",
    "    remaining_entropy = 0\n",
    "\n",
    "    for feature in feature_list:\n",
    "        remaining_entropy = rem_entropy(feature, D, target_feature)\n",
    "        IG_dict[feature] = tf_entropy - remaining_entropy\n",
    "    return return_highest_ig_feature(IG_dict)\n",
    "\n",
    "\n",
    "def rem_entropy(feature, D, target_feature) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the remaing entropy of a dataframe on a specific feature and it's values\n",
    "    @param feature: string\n",
    "    @param D: dataframe,\n",
    "    @param target_feature: String\n",
    "    @return: float\n",
    "    \"\"\"\n",
    "    rem_en = 0\n",
    "    feature_value_list = list(D[feature].unique())\n",
    "    target_feature_list = D[target_feature].unique()\n",
    "\n",
    "    for feature_value in feature_value_list:\n",
    "        temp = 0\n",
    "\n",
    "        for value in target_feature_list:\n",
    "            P = len(D[(D[feature] == feature_value) & (D[target_feature] == value)]) / len(\n",
    "                D[D[feature] == feature_value])\n",
    "\n",
    "            if P != 0.0:\n",
    "                temp += P * m.log(P, 2)\n",
    "        temp *= -1\n",
    "        temp *= len(D[D[feature] == feature_value]) / len(D)\n",
    "        rem_en += temp\n",
    "    # send to calculate entropy\n",
    "    # multiply returned entropy by P(target_feature/D)\n",
    "    return rem_en\n",
    "\n",
    "\n",
    "def calculate_entropy(target_feature, D) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a feature\n",
    "    @param target_feature: String\n",
    "    @param D: dataframe\n",
    "    @return: float\n",
    "    \"\"\"\n",
    "    entropy = 0\n",
    "    # partition DS on target_feature\n",
    "    # list of feature values\n",
    "    feature_values_list = D[target_feature].unique()\n",
    "\n",
    "    for feature in feature_values_list:\n",
    "        entropy += (len(D[D[target_feature] == feature]) / len(D)) * m.log(\n",
    "            len(D[D[target_feature] == feature]) / len(D), 2)\n",
    "    # partition DS on tf features and get probabilities for entropy calculation\n",
    "    # calculate entropy\n",
    "    return -1 * entropy\n",
    "\n",
    "\n",
    "def return_highest_ig_feature(ig_dict) -> str:\n",
    "    \"\"\"\n",
    "    returns the highest info gain feature\n",
    "    @param ig_dict: dictionary\n",
    "    @return: str\n",
    "    \"\"\"\n",
    "    return max(ig_dict, key=ig_dict.get)\n",
    "\n",
    "\n",
    "def ID3(d, D, target_value, feature=None, previous_value=None) -> dict:\n",
    "    \"\"\"\n",
    "    builds a decision tree by partitioning the dataset based on the highest IG feature in that dataset\n",
    "    @param d: list of features\n",
    "    @param D: dataframe\n",
    "    @param target_value: string\n",
    "    @param feature: string(optional )\n",
    "    @param previous_value: string(optional)\n",
    "    @return: dictionary\n",
    "    \"\"\"\n",
    "    tree = {}\n",
    "\n",
    "    # remove the target value, will only be done before recursion begins\n",
    "    if d.__contains__(target_value):\n",
    "        d.remove(target_value)\n",
    "\n",
    "    # if the partitioned df comes back empty then return the past max target value\n",
    "    if D.empty:\n",
    "        return previous_value\n",
    "#     if len(D) <= 30:\n",
    "#         return previous_value\n",
    "\n",
    "    # if only one target value feature exists in the dataframe, return that value\n",
    "    if len(list(D[target_value].unique())) == 1:\n",
    "        return list(D[target_value].unique())[0]\n",
    "    # TODO: This might break :)\n",
    "    # if feature list only contains one value return the most common target value feature\n",
    "    elif len(d) == 1:\n",
    "        # return D[D[previous_value]][target_value].mode()\n",
    "        return D[target_value].mode()[0]\n",
    "    # if feature list is empty then return the max target value of the current feature as a dictionary\n",
    "    elif len(d) == 0:\n",
    "        max_target_value = max(D.groupby(target_value).count().to_dict(orient=\"dict\")[feature],\n",
    "                               key=D.groupby(target_value).count().to_dict(orient=\"dict\")[\n",
    "                                   feature].get)\n",
    "        tree[feature] = max_target_value\n",
    "        return tree\n",
    "    # Else build tree via recursion\n",
    "    else:\n",
    "        # get past value before partition changes, need to keep track of this everytime\n",
    "        past_value = D[target_value].mode()[0]\n",
    "        # sub_tree for building\n",
    "        sub_tree = {}\n",
    "\n",
    "        # get the max information gain value\n",
    "        max_IG_value = IG(target_value, D)\n",
    "\n",
    "        # remove the value from the possible features list\n",
    "        d.remove(max_IG_value)\n",
    "\n",
    "        # for each category in parition of max_IG_Value, build sub tree\n",
    "        #   then cry because this took 5 hours to figure out and you've finally got it done\n",
    "        list_of_values = list(D[max_IG_value].unique())\n",
    "        for value in list_of_values:\n",
    "            new_D = D[D[max_IG_value] == value]\n",
    "            new_D = new_D.drop(columns=[max_IG_value], axis=1)\n",
    "\n",
    "            # send shallow copy of d feature list or else bad things happen\n",
    "            sub_tree[value] = ID3(d.copy(), new_D, target_value, max_IG_value,past_value)\n",
    "        previous_value = max_IG_value\n",
    "\n",
    "        # link the sub tree to the tree so as not to lose every other layer\n",
    "        tree[max_IG_value] = sub_tree\n",
    "    return tree\n",
    "\n",
    "\n",
    "def validate(tree_dictionary, data) -> str:\n",
    "    \"\"\"\n",
    "    test tree path against test data to determine accuracy\n",
    "    @param tree_dictionary: dictionary/decision tree\n",
    "    @param data: row from dataframe to test\n",
    "    @return: target value of row path through tree\n",
    "    \"\"\"\n",
    "\n",
    "    # if it's not a dicitonary return it\n",
    "    if not isinstance(tree_dictionary, dict):\n",
    "        return tree_dictionary\n",
    "    # else: value = rootnode of tree, tree = whatever the root-nodes value is\n",
    "    value = list(tree_dictionary.keys())[0]\n",
    "    tree = tree_dictionary.get(value)\n",
    "\n",
    "    # get the column feature\n",
    "    value_attribute = data[value]\n",
    "\n",
    "    # do the recursion to walk the tree\n",
    "    return validate(tree.get(value_attribute), data)\n",
    "\n",
    "\n",
    "def accuracy(model, test_set, target_feature):\n",
    "    \"\"\"\n",
    "    traverse through test set rows and test for accuracy on returned values from validation\n",
    "    @param model: decision tree/dicitonary\n",
    "    @param test_set: dataframe\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for ind in range(0, len(test_set.index)):\n",
    "        guess = validate(model, test_set.loc[ind])\n",
    "        # if guess == target feature value of specific row being tested\n",
    "        if guess == test_set.loc[ind][target_feature]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    print(\"Accuracy: \", f'{correct / (len(test_set.index)): .2%}')\n",
    "\n",
    "\n",
    "def testing_testing(fld, file, td_file_path, target_feature):\n",
    "    \"\"\"\n",
    "    do the pre-training stuff, like reading in the file, grabbing the pickled model from the jar\n",
    "    @param fld: folder name\n",
    "    @param file: file name\n",
    "    @param td_file_path: file path of the testing data\n",
    "    @param target_feature: target feature\n",
    "    \"\"\"\n",
    "    training_data = read_file(td_file_path)\n",
    "    my_tree_model = pickle.load(open(os.path.join(fld, file), 'rb'))\n",
    "    accuracy(my_tree_model, training_data, target_feature)\n",
    "\n",
    "\n",
    "\n",
    "dataset = read_file(\"C:\\\\Users\\\\dayna\\\\Desktop\\\\Github\\\\NoStigma-HackSEA\\\\data\\\\trainingData.csv\")\n",
    "#model = ID3(list(dataset.columns), dataset, \"result\")\n",
    "#print(model)\n",
    "\n",
    "pruned_model = ID3(list(dataset.columns), dataset, \"result\")\n",
    "\n",
    "\n",
    "#print(list(dataset.columns))\n",
    "\n",
    "folder_name = \"Models\"\n",
    "#file_name = \"prePrunedModel\"\n",
    "pruned_file_name = \"noStigmaModel\"\n",
    "tf = \"result\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "#pickle.dump(model, open(os.path.join(folder_name,file_name), 'wb'), protocol=4)\n",
    "pickle.dump(pruned_model, open(os.path.join(folder_name,pruned_file_name), 'wb'), protocol=4)\n",
    "\n",
    "\n",
    "print(\"=================Starting Test Pre-Pruning=================\")\n",
    "testing_testing(folder_name, file_name,\n",
    "                \"C:\\\\Users\\\\dayna\\\\Desktop\\\\Github\\\\NoStigma-HackSEA\\\\data\\\\trainingData.csv\", tf)\n",
    "print(\"=================End Test=================\")\n",
    "print(\"=================Starting Test Post-Pruning=================\")\n",
    "testing_testing(folder_name, pruned_file_name,\n",
    "                \"C:\\\\Users\\\\dayna\\\\Desktop\\\\Github\\\\NoStigma-HackSEA\\\\data\\\\trainingData.csv\", tf)\n",
    "print(\"=================End Test=================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf53f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
