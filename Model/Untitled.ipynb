{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6c008f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\dayna\\\\Desktop\\\\Github\\\\NoStigma-HackSEA\\\\data\\\\trainingData.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 221>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    214\u001b[0m     accuracy(my_tree_model, training_data, target_feature)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# dataset = read_file(\"C:\\\\Users\\\\Dayna\\\\OneDrive - Bellevue College\\\\ML\\\\Assignment1\\\\dayna_clifford_assignment1\\\\assets\\\\playtennis.csv\")\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# dataset = read_file(\"C:\\\\Users\\\\Dayna\\\\Desktop\\\\Fall2022\\\\CS460\\\\Assignment2\\\\data\\\\emails.csv\")\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m#dataset = read_file(\"C:\\\\Users\\\\Dayna\\\\Desktop\\\\Fall2022\\\\CS460\\\\Assignment2\\\\data\\\\census_training.csv\")\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdayna\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mGithub\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mNoStigma-HackSEA\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtrainingData.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m#model = ID3(list(dataset.columns), dataset, \"result\")\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(file_path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\dayna\\\\Desktop\\\\Github\\\\NoStigma-HackSEA\\\\data\\\\trainingData.txt'"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def read_file(file_path) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def IG(target_feature, D, remove=\"null\") -> dict:\n",
    "    \"\"\"\n",
    "    Calculates information gain by partitioning D on features and calculating their entropy, then summing it all together\n",
    "    and subtracting that from the target feature entropy\n",
    "    @param target_feature: string, The desired outcome\n",
    "    @param D: dataframe, dataset needed to perform entropy calculations\n",
    "    @param remove: string, name of column needed to be removed from dataframe before entropy calculations. Optional\n",
    "    @return: the highest Information Gain feature\n",
    "    \"\"\"\n",
    "    IG_dict = {}\n",
    "    # partition on target feature\n",
    "    # calculate target feature entropy\n",
    "    if remove != \"null\":\n",
    "        D.drop(columns=['Day'], axis=1, inplace=True)\n",
    "    tf_entropy = calculate_entropy(target_feature, D)\n",
    "\n",
    "    feature_list = list(D.columns)\n",
    "\n",
    "    feature_list.remove(target_feature)\n",
    "\n",
    "    remaining_entropy = 0\n",
    "\n",
    "    for feature in feature_list:\n",
    "        remaining_entropy = rem_entropy(feature, D, target_feature)\n",
    "        IG_dict[feature] = tf_entropy - remaining_entropy\n",
    "    return return_highest_ig_feature(IG_dict)\n",
    "\n",
    "\n",
    "def rem_entropy(feature, D, target_feature) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the remaing entropy of a dataframe on a specific feature and it's values\n",
    "    @param feature: string\n",
    "    @param D: dataframe,\n",
    "    @param target_feature: String\n",
    "    @return: float\n",
    "    \"\"\"\n",
    "    rem_en = 0\n",
    "    feature_value_list = list(D[feature].unique())\n",
    "    target_feature_list = D[target_feature].unique()\n",
    "\n",
    "    for feature_value in feature_value_list:\n",
    "        temp = 0\n",
    "\n",
    "        for value in target_feature_list:\n",
    "            P = len(D[(D[feature] == feature_value) & (D[target_feature] == value)]) / len(\n",
    "                D[D[feature] == feature_value])\n",
    "\n",
    "            if P != 0.0:\n",
    "                temp += P * m.log(P, 2)\n",
    "        temp *= -1\n",
    "        temp *= len(D[D[feature] == feature_value]) / len(D)\n",
    "        rem_en += temp\n",
    "    # send to calculate entropy\n",
    "    # multiply returned entropy by P(target_feature/D)\n",
    "    return rem_en\n",
    "\n",
    "\n",
    "def calculate_entropy(target_feature, D) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a feature\n",
    "    @param target_feature: String\n",
    "    @param D: dataframe\n",
    "    @return: float\n",
    "    \"\"\"\n",
    "    entropy = 0\n",
    "    # partition DS on target_feature\n",
    "    # list of feature values\n",
    "    feature_values_list = D[target_feature].unique()\n",
    "\n",
    "    for feature in feature_values_list:\n",
    "        entropy += (len(D[D[target_feature] == feature]) / len(D)) * m.log(\n",
    "            len(D[D[target_feature] == feature]) / len(D), 2)\n",
    "    # partition DS on tf features and get probabilities for entropy calculation\n",
    "    # calculate entropy\n",
    "    return -1 * entropy\n",
    "\n",
    "\n",
    "def return_highest_ig_feature(ig_dict) -> str:\n",
    "    \"\"\"\n",
    "    returns the highest info gain feature\n",
    "    @param ig_dict: dictionary\n",
    "    @return: str\n",
    "    \"\"\"\n",
    "    return max(ig_dict, key=ig_dict.get)\n",
    "\n",
    "\n",
    "def ID3(d, D, target_value, feature=None, previous_value=None) -> dict:\n",
    "    \"\"\"\n",
    "    builds a decision tree by partitioning the dataset based on the highest IG feature in that dataset\n",
    "    @param d: list of features\n",
    "    @param D: dataframe\n",
    "    @param target_value: string\n",
    "    @param feature: string(optional )\n",
    "    @param previous_value: string(optional)\n",
    "    @return: dictionary\n",
    "    \"\"\"\n",
    "    print(feature)\n",
    "    tree = {}\n",
    "\n",
    "    # remove the target value, will only be done before recursion begins\n",
    "    if d.__contains__(target_value):\n",
    "        d.remove(target_value)\n",
    "\n",
    "    # if the partitioned df comes back empty then return the past max target value\n",
    "    if D.empty:\n",
    "        return previous_value\n",
    "    # if len(D) <= 30:\n",
    "    #     return previous_value\n",
    "\n",
    "    # if only one target value feature exists in the dataframe, return that value\n",
    "    if len(list(D[target_value].unique())) == 1:\n",
    "        return list(D[target_value].unique())[0]\n",
    "    # TODO: This might break :)\n",
    "    # if feature list only contains one value return the most common target value feature\n",
    "    elif len(d) == 1:\n",
    "        # return D[D[previous_value]][target_value].mode()\n",
    "        return D[target_value].mode()[0]\n",
    "    # if feature list is empty then return the max target value of the current feature as a dictionary\n",
    "    elif len(d) == 0:\n",
    "        max_target_value = max(D.groupby(target_value).count().to_dict(orient=\"dict\")[feature],\n",
    "                               key=D.groupby(target_value).count().to_dict(orient=\"dict\")[\n",
    "                                   feature].get)\n",
    "        tree[feature] = max_target_value\n",
    "        return tree\n",
    "    # Else build tree via recursion\n",
    "    else:\n",
    "        # get past value before partition changes, need to keep track of this everytime\n",
    "        past_value = D[target_value].mode()[0]\n",
    "        # sub_tree for building\n",
    "        sub_tree = {}\n",
    "\n",
    "        # get the max information gain value\n",
    "        max_IG_value = IG(target_value, D)\n",
    "\n",
    "        # remove the value from the possible features list\n",
    "        d.remove(max_IG_value)\n",
    "\n",
    "        # for each category in parition of max_IG_Value, build sub tree\n",
    "        #   then cry because this took 5 hours to figure out and you've finally got it done\n",
    "        list_of_values = list(D[max_IG_value].unique())\n",
    "        for value in list_of_values:\n",
    "            new_D = D[D[max_IG_value] == value]\n",
    "            new_D = new_D.drop(columns=[max_IG_value], axis=1)\n",
    "\n",
    "            # send shallow copy of d feature list or else bad things happen\n",
    "            sub_tree[value] = ID3(d.copy(), new_D, target_value, max_IG_value, value, past_value)\n",
    "        previous_value = max_IG_value\n",
    "\n",
    "        # link the sub tree to the tree so as not to lose every other layer\n",
    "        tree[max_IG_value] = sub_tree\n",
    "    return tree\n",
    "\n",
    "\n",
    "def validate(tree_dictionary, data) -> str:\n",
    "    \"\"\"\n",
    "    test tree path against test data to determine accuracy\n",
    "    @param tree_dictionary: dictionary/decision tree\n",
    "    @param data: row from dataframe to test\n",
    "    @return: target value of row path through tree\n",
    "    \"\"\"\n",
    "\n",
    "    # if it's not a dicitonary return it\n",
    "    if not isinstance(tree_dictionary, dict):\n",
    "        return tree_dictionary\n",
    "    # else: value = rootnode of tree, tree = whatever the root-nodes value is\n",
    "    value = list(tree_dictionary.keys())[0]\n",
    "    tree = tree_dictionary.get(value)\n",
    "\n",
    "    # get the column feature\n",
    "    value_attribute = data[value]\n",
    "\n",
    "    # do the recursion to walk the tree\n",
    "    return validate(tree.get(value_attribute), data)\n",
    "\n",
    "\n",
    "def accuracy(model, test_set, target_feature):\n",
    "    \"\"\"\n",
    "    traverse through test set rows and test for accuracy on returned values from validation\n",
    "    @param model: decision tree/dicitonary\n",
    "    @param test_set: dataframe\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for ind in range(0, len(test_set.index)):\n",
    "        guess = validate(model, test_set.loc[ind])\n",
    "        # if guess == target feature value of specific row being tested\n",
    "        if guess == test_set.loc[ind][target_feature]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    print(\"Accuracy: \", f'{correct / (len(test_set.index)): .2%}')\n",
    "\n",
    "\n",
    "def testing_testing(fld, file, td_file_path, target_feature):\n",
    "    \"\"\"\n",
    "    do the pre-training stuff, like reading in the file, grabbing the pickled model from the jar\n",
    "    @param fld: folder name\n",
    "    @param file: file name\n",
    "    @param td_file_path: file path of the testing data\n",
    "    @param target_feature: target feature\n",
    "    \"\"\"\n",
    "    training_data = read_file(td_file_path)\n",
    "    my_tree_model = pickle.load(open(os.path.join(fld, file), 'rb'))\n",
    "    accuracy(my_tree_model, training_data, target_feature)\n",
    "\n",
    "\n",
    "# dataset = read_file(\"C:\\\\Users\\\\Dayna\\\\OneDrive - Bellevue College\\\\ML\\\\Assignment1\\\\dayna_clifford_assignment1\\\\assets\\\\playtennis.csv\")\n",
    "# dataset = read_file(\"C:\\\\Users\\\\Dayna\\\\Desktop\\\\Fall2022\\\\CS460\\\\Assignment2\\\\data\\\\emails.csv\")\n",
    "#dataset = read_file(\"C:\\\\Users\\\\Dayna\\\\Desktop\\\\Fall2022\\\\CS460\\\\Assignment2\\\\data\\\\census_training.csv\")\n",
    "\n",
    "dataset = read_file(\"C:\\\\Users\\\\dayna\\\\Desktop\\\\Github\\\\NoStigma-HackSEA\\\\data\\\\trainingData.txt\")\n",
    "#model = ID3(list(dataset.columns), dataset, \"result\")\n",
    "\n",
    "print(list(dataset.columns))\n",
    "\n",
    "# folder_name = \"model\"\n",
    "# file_name = \"censusModel\"\n",
    "# pp_file_name = \"post_pruning_model\"\n",
    "# tf = \"high_income\"\n",
    "#\n",
    "# print(\"=================Starting Test Pre-Pruning=================\")\n",
    "# testing_testing(folder_name, file_name,\n",
    "#                 \"C:\\\\Users\\\\Dayna\\\\Desktop\\\\Fall2022\\\\CS460\\\\Assignment2\\\\data\\\\census_training_test.csv\", tf)\n",
    "# print(\"=================End Test=================\")\n",
    "# print(\"=================Starting Test Post-Pruning=================\")\n",
    "# testing_testing(folder_name, pp_file_name,\n",
    "#                 \"C:\\\\Users\\\\Dayna\\\\Desktop\\\\Fall2022\\\\CS460\\\\Assignment2\\\\data\\\\census_training_test.csv\", tf)\n",
    "# print(\"=================End Test=================\")\n",
    "\n",
    "# if not os.path.exists(folder_name):\n",
    "#     os.makedirs(folder_name)\n",
    "\n",
    "# census_model = ID3(list(dataset.columns), dataset, \"high_income\")\n",
    "# post_pruning_model = ID3(list(dataset.columns), dataset, \"high_income\")\n",
    "# training_data = read_file(\"C:\\\\Users\\\\Dayna\\\\Desktop\\\\Fall2022\\\\CS460\\\\Assignment2\\\\data\\\\census_training_test.csv\")\n",
    "# pickle.dump(census_model, open(os.path.join(folder_name,file_name), 'wb'), protocol=4)\n",
    "# pickle.dump(post_pruning_model, open(os.path.join(folder_name,file_name), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb73d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
